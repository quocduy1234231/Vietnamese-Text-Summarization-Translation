{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11652391,"sourceType":"datasetVersion","datasetId":7312606}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rouge-score ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:43:23.404690Z","iopub.execute_input":"2025-05-02T23:43:23.404944Z","iopub.status.idle":"2025-05-02T23:43:29.074777Z","shell.execute_reply.started":"2025-05-02T23:43:23.404923Z","shell.execute_reply":"2025-05-02T23:43:29.073748Z"}},"outputs":[{"name":"stdout","text":"Collecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge-score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge-score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge-score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge-score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge-score) (2024.2.0)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=9979edbc04669f37184044f59c0e54eab4f6e70a17fd39f6ee4962ed325636f2\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport torch\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom rouge_score import rouge_scorer\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:43:29.076609Z","iopub.execute_input":"2025-05-02T23:43:29.076856Z","iopub.status.idle":"2025-05-02T23:43:34.480798Z","shell.execute_reply.started":"2025-05-02T23:43:29.076827Z","shell.execute_reply":"2025-05-02T23:43:34.480270Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Đọc dữ liệu từ file\nwith open('/kaggle/input/dataset/en_sents_dataset.txt', 'r', encoding='utf-8') as f:\n    en = [line.strip() for line in f.readlines() if line.strip()]\nwith open('/kaggle/input/dataset/vi_sents_dataset.txt', 'r', encoding='utf-8') as f:\n    vi = [line.strip() for line in f.readlines() if line.strip()]\nassert len(en) == len(vi), \"Số câu EN và VI không khớp!\"\n\n# Chia bộ dữ liệu thành Train/Val/Test\nen_train, en_temp, vi_train, vi_temp = train_test_split(en, vi, test_size=0.2, random_state=42)\nen_val, en_test, vi_val, vi_test = train_test_split(en_temp, vi_temp, test_size=0.5, random_state=42)\n\nprint(f\"Train: {len(en_train)} | Val: {len(en_val)} | Test: {len(en_test)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:43:34.481489Z","iopub.execute_input":"2025-05-02T23:43:34.481774Z","iopub.status.idle":"2025-05-02T23:43:38.584569Z","shell.execute_reply.started":"2025-05-02T23:43:34.481756Z","shell.execute_reply":"2025-05-02T23:43:38.583813Z"}},"outputs":[{"name":"stdout","text":"Train: 597182 | Val: 74648 | Test: 74648\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"vocab_size = 30000\nspecial_tokens = ['[PAD]', '[UNK]', '[BOS]', '[EOS]']\n\n# Train tokenizer\ndef train_tokenizer(txt_file, save_path):\n    tokenizer = Tokenizer(models.BPE())\n    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n    tokenizer.decoder = decoders.ByteLevel()\n    trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=special_tokens)\n    tokenizer.train([txt_file], trainer)\n    tokenizer.save(save_path)\n    return tokenizer\n\n# Tạo thư mục và train tokenizer cho tiếng Anh và tiếng Việt\nos.makedirs(\"checkpoints\", exist_ok=True)\ntokenizer_src = train_tokenizer(\"/kaggle/input/dataset/en_sents_dataset.txt\", \"checkpoints/tokenizer_src.json\")\ntokenizer_tgt = train_tokenizer(\"/kaggle/input/dataset/vi_sents_dataset.txt\", \"checkpoints/tokenizer_tgt.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:43:38.586438Z","iopub.execute_input":"2025-05-02T23:43:38.586657Z","iopub.status.idle":"2025-05-02T23:44:13.145505Z","shell.execute_reply.started":"2025-05-02T23:43:38.586640Z","shell.execute_reply":"2025-05-02T23:44:13.144715Z"}},"outputs":[{"name":"stdout","text":"\n\n\n\n\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Định nghĩa mô hình GPT\nclass GPTBlock(nn.Module):\n    def __init__(self, dim, n_heads, ff_dim, dropout):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(dim)\n        self.attn = nn.MultiheadAttention(dim, n_heads, dropout=dropout, batch_first=True)\n        self.ln2 = nn.LayerNorm(dim)\n        self.ff = nn.Sequential(\n            nn.Linear(dim, ff_dim),\n            nn.GELU(),\n            nn.Linear(ff_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x, mask):\n        x = x + self.attn(self.ln1(x), self.ln1(x), self.ln1(x), attn_mask=mask)[0]\n        return x + self.ff(self.ln2(x))\n\nclass GPT2Translator(nn.Module):\n    def __init__(self, vocab_size, dim=256, n_layers=4, n_heads=4, ff_dim=1024, max_len=128, dropout=0.1):\n        super().__init__()\n        self.token_emb = nn.Embedding(vocab_size, dim)\n        self.pos_emb = nn.Embedding(max_len, dim)\n        self.blocks = nn.ModuleList([\n            GPTBlock(dim, n_heads, ff_dim, dropout) for _ in range(n_layers)\n        ])\n        self.ln = nn.LayerNorm(dim)\n        self.out = nn.Linear(dim, vocab_size)\n\n    def forward(self, x):\n        B, T = x.shape\n        pos = torch.arange(0, T, device=x.device).unsqueeze(0)\n        x = self.token_emb(x) + self.pos_emb(pos)\n        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n        for block in self.blocks:\n            x = block(x, mask)\n        return self.out(self.ln(x))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:44:13.147306Z","iopub.execute_input":"2025-05-02T23:44:13.147539Z","iopub.status.idle":"2025-05-02T23:44:13.245454Z","shell.execute_reply.started":"2025-05-02T23:44:13.147521Z","shell.execute_reply":"2025-05-02T23:44:13.244757Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Khởi tạo thiết bị (CPU/GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer_src = Tokenizer.from_file(\"checkpoints/tokenizer_src.json\")\ntokenizer_tgt = Tokenizer.from_file(\"checkpoints/tokenizer_tgt.json\")\npad_id = tokenizer_tgt.token_to_id(\"[PAD]\")\nbos_id = tokenizer_tgt.token_to_id(\"[BOS]\")\neos_id = tokenizer_tgt.token_to_id(\"[EOS]\")\n\n# Khởi tạo mô hình\nmodel = GPT2Translator(vocab_size=tokenizer_tgt.get_vocab_size()).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n\n# Hàm mã hóa cặp câu (Bước tiền xử lý quan trọng)\ndef encode_pair(en_sent, vi_sent, max_len=128):\n    # Tokenize câu tiếng Anh và tiếng Việt\n    src_ids = tokenizer_src.encode(en_sent.lower()).ids[:max_len]\n    tgt_ids = tokenizer_tgt.encode(vi_sent.lower()).ids[:max_len-1]\n    \n    # Thêm token BOS cho câu đích và EOS\n    tgt_ids = [bos_id] + tgt_ids + [eos_id]\n    tgt_ids = tgt_ids[:max_len]\n    \n    # Padding để đảm bảo chiều dài cố định\n    src_ids += [pad_id] * (max_len - len(src_ids))\n    tgt_ids += [pad_id] * (max_len - len(tgt_ids))\n    \n    return torch.tensor(src_ids), torch.tensor(tgt_ids)\n\n# Hàm lấy batch\ndef get_batch(en_list, vi_list, batch_size=32):\n    idxs = random.sample(range(len(en_list)), batch_size)\n    srcs, tgts = [], []\n    for i in idxs:\n        src, tgt = encode_pair(en_list[i], vi_list[i])\n        srcs.append(src)\n        tgts.append(tgt)\n    return torch.stack(srcs).to(device), torch.stack(tgts).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:44:13.246147Z","iopub.execute_input":"2025-05-02T23:44:13.246331Z","iopub.status.idle":"2025-05-02T23:44:16.528076Z","shell.execute_reply.started":"2025-05-02T23:44:13.246312Z","shell.execute_reply":"2025-05-02T23:44:16.527301Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nimport time\n\n# Thiết bị huấn luyện\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Khởi tạo các biến cho early stopping\nbest_val_loss = float('inf')\nno_improve_counter = 0\nearly_stopping_patience = 2  # Số lần cho phép không cải thiện trước khi dừng\n\nbatch_size = 32\n\n# Huấn luyện mô hình\nfor epoch in range(5):\n    model.train()\n    total_loss = 0\n    start_time = time.time()\n\n    for _ in tqdm(range(len(en_train)//batch_size), desc=f\"Epoch {epoch+1}\"):\n        src, tgt = get_batch(en_train, vi_train)\n        src, tgt = src.to(device), tgt.to(device)\n\n        logits = model(src)\n        loss = criterion(logits.view(-1, logits.size(-1)), tgt.view(-1))\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n    \n    avg_train_loss = total_loss / (len(en_train)//batch_size)\n    print(f\"Epoch {epoch+1} - Loss: {avg_train_loss:.4f}\")\n\n    # Validation\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for _ in tqdm(range(len(en_val)//batch_size), desc=\"Validation\"):\n            src, tgt = get_batch(en_val, vi_val)\n            src, tgt = src.to(device), tgt.to(device)\n\n            logits = model(src)\n            loss = criterion(logits.view(-1, logits.size(-1)), tgt.view(-1))\n            val_loss += loss.item()\n\n    avg_val_loss = val_loss / (len(en_val)//batch_size)\n    print(f\"Epoch {epoch+1} - Validation Loss: {avg_val_loss:.4f}\")\n    print(f\"Epoch {epoch+1} - Time: {time.time() - start_time:.2f} sec\")\n\n    # Early stopping\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        torch.save(model.state_dict(), f\"checkpoints/gpt2_translator_epoch{epoch+1}.pth\")\n        no_improve_counter = 0\n    else:\n        no_improve_counter += 1\n        if no_improve_counter >= early_stopping_patience:\n            print(f\"Early stopping after {epoch+1} epochs\")\n            break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T23:44:16.528960Z","iopub.execute_input":"2025-05-02T23:44:16.529265Z","iopub.status.idle":"2025-05-03T02:51:53.549127Z","shell.execute_reply.started":"2025-05-02T23:44:16.529241Z","shell.execute_reply":"2025-05-03T02:51:53.548491Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 18661/18661 [35:39<00:00,  8.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Loss: 5.4953\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 2332/2332 [01:44<00:00, 22.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Validation Loss: 5.0072\nEpoch 1 - Time: 2243.89 sec\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 18661/18661 [35:47<00:00,  8.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Loss: 4.7719\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 2332/2332 [01:44<00:00, 22.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Validation Loss: 4.6155\nEpoch 2 - Time: 2251.89 sec\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 18661/18661 [35:50<00:00,  8.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Loss: 4.4654\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 2332/2332 [01:44<00:00, 22.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Validation Loss: 4.4181\nEpoch 3 - Time: 2254.71 sec\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 18661/18661 [35:47<00:00,  8.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Loss: 4.2896\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 2332/2332 [01:44<00:00, 22.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Validation Loss: 4.2949\nEpoch 4 - Time: 2252.49 sec\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 18661/18661 [35:48<00:00,  8.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Loss: 4.1665\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 2332/2332 [01:44<00:00, 22.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Validation Loss: 4.2169\nEpoch 5 - Time: 2253.32 sec\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def generate_translation(model, src_sentence, max_len=128):\n    model.eval()\n    with torch.no_grad():\n        src_ids = tokenizer_src.encode(src_sentence.lower()).ids[:max_len]\n        src_ids += [pad_id] * (max_len - len(src_ids))\n        src_tensor = torch.tensor([src_ids], device=device)\n\n        # Bắt đầu với BOS token\n        generated = [bos_id]\n        for _ in range(max_len):\n            tgt_tensor = torch.tensor([generated + [pad_id] * (max_len - len(generated))], device=device)\n            logits = model(src_tensor)\n            next_token = torch.argmax(logits[0, len(generated)-1]).item()\n\n            # Dừng nếu gặp EOS\n            if next_token == eos_id:\n                break\n            generated.append(next_token)\n\n        # Giải mã thành câu\n        decoded = tokenizer_tgt.decode(generated[1:])  # Bỏ BOS\n        return decoded\n# Dịch thử 15 câu đầu trong tập test\nfor i in range(15):\n    src_sentence = en_test[i]\n    ref_sentence = vi_test[i]\n    pred_sentence = generate_translation(model, src_sentence)\n\n    print(f\"\\nInput : {src_sentence}\")\n    print(f\"Target: {ref_sentence}\")\n    print(f\"Pred  : {pred_sentence}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T03:12:18.605892Z","iopub.execute_input":"2025-05-03T03:12:18.606165Z","iopub.status.idle":"2025-05-03T03:12:19.743979Z","shell.execute_reply.started":"2025-05-03T03:12:18.606145Z","shell.execute_reply":"2025-05-03T03:12:19.743230Z"}},"outputs":[{"name":"stdout","text":"\nInput : Some people turn into rockers like this.\nTarget: Thực kinh khủng. Một vài người trở thành những rocker như thế.\nPred  :  một người người người vào đá như như như này\n\nInput : As had so often happened on the Eastern Front Hitler refused to allow a strategic withdrawal until it was too late.\nTarget: \"Tương tự như mặt trận phía đông, Hitler lại không cho phép rút quân cho đến khi quá trễ.\"\nPred  :  \" như như thường ra ra ra phía phía phía hit phía phía chối chối phép cho cho rút chiến rút rút cho cho đến đến đến.\n\nInput : \"At 290 days old males are about 14.1 kg, and females are about 12.6 kg.\"\nTarget: \"Khi 290 ngày tuổi, con đực đạt chừng 14,1 kg, còn con cái đạt khoảng 12.6 kg.\"\nPred  :  \"vào 290 290 tuổi tuổi đực tuổi tuổi 14... kg, con con 12.6.666.\".\".\"\n\nInput : \"As a result, costs in the amount of 14 million euros were assumed.\"\nTarget: \"Tuy nhiên, ước tính mức phí là khoảng 14 triệu euro.\"\nPred  :  \"làết quả, phí phí trong lượng số triệu, 14 được được được được.\".\"\n\nInput : We should do this again sometime.\nTarget: đôi khi chúng ta nên làm điều này một lần nữa\nPred  :  chúng ta nên làm điều lần lần lần\n\nInput : How big a chance is there that Tom will come?\nTarget: Tom sẽ đến như thế nào?\nPred  :  bạn thế lớn cơ cơ cơ có có tom đến đến đến\n\nInput : \"Tom went swimming in the river, but when he got out, his clothes had been stolen\"\nTarget: \"Tom đã đi bơi dưới sông, nhưng khi anh ta ra ngoài, quần áo của anh ta đã bị đánh cắp\"\nPred  :  \"tom đi bơi bơi trong sông, nhưng nhưng nhưng anh ra ra, quần áo đã áo\n\nInput : The heaviest artillery attack was on December 6 with 19 people killed and 60 wounded.\nTarget: Cuộc tấn công bằng pháo lớn nhất xay ra vào ngày 6 tháng 12 với 19 người bị giết và 60 người bị thương.\nPred  :  những pháo tấn lớn lớn vào vào vào tháng tháng tháng 12 19 thiệt mạng mạng mạng người người thương thương thương thương thương\n\nInput : \"You could say that the area was converging, approaching a fixed number, while the perimeter was just getting bigger and bigger, uncontrollably ballooning like an overindulgent birthday clown.\"\nTarget: \"Diện tích của nó đang hội tụ, tiệm cận một con số xác đinh, trong khi chu vi sẽ luôn đạt tới một số lớn hơn, bành trướng tới vô hạn như một chú hề càn rỡ trong buổi sinh nhật.\"\nPred  :  \"bạn có thể nói rằng rằng vực vực được,,,, số số cố cố cố khi khi khi khi khi và và và và,,,,,,, như như như hơn như như như\n\nInput : So in summer 2000 I was the first to BASE jump the Eiger North Face in Switzerland.\nTarget: \"Vào mùa hè 2000 tôi là người thực hiện cú BASE jump ở Eiger North Face, Thụy Sĩ.\"\nPred  :  \" thế năm hè 2000 2000, tôi tiên tiên tiên tiên tiên bắc e bắc bắc bắc ở.\n\nInput : They went to a fortune teller.\nTarget: họ đã đi đến một thầy bói.\nPred  :  họ đã đến một cho cho cho..\n\nInput : My uncle has a large family.\nTarget: chú tôi có một gia đình lớn.\nPred  :  chú tôi có có đình đình lớn\n\nInput : \"Approximately 40,000 hectares of land are cultivated in Oudomxay, with rice being the main crop.\"\nTarget: \"Có khoảng 40.000 ha canh tác ở Oudomxay, lúa là cây trồng chính.\"\nPred  :  \"khoảng 40.000000 đất đất đất đất o oom,ay,,,, là là cây.\"\n\nInput : \"They cook with it, clean with it, even make their medicines with it.\"\nTarget: \"Họ nấu ăn bằng nước của dòng suối, tắm rửa, thậm chí dùng nó làm thuốc.\"\nPred  :  \"họ nấu nấu với nó, sạch nó nó,,,, thuốc thuốc thuốc thuốc\n\nInput : She returns with Jack to her childhood home where her mother and Leo reside.\nTarget: Cô cùng Jack trở về nhà nơi mẹ cô và Leo sống.\nPred  :  cô trở lại với với với về về ấu mẹ và và và mẹ và và\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom rouge_score import rouge_scorer\n\nsmoothie = SmoothingFunction().method4\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)\n\nbleu_scores = []\nrouge1_scores = []\nrouge2_scores = []\nrougeL_scores = []\nrougeLsum_scores = []\n\n# Đánh giá trên 500 câu test đầu tiên\nfor i in range(500):\n    src_sent = en_test[i]\n    ref_sent = vi_test[i]\n    pred_sent = generate_translation(model, src_sent)\n\n    # Tính BLEU\n    bleu = sentence_bleu(\n        [ref_sent.split()],\n        pred_sent.split(),\n        smoothing_function=smoothie\n    )\n    bleu_scores.append(bleu * 100)\n\n    # Tính ROUGE\n    scores = scorer.score(ref_sent, pred_sent)\n    rouge1_scores.append(scores['rouge1'].fmeasure)\n    rouge2_scores.append(scores['rouge2'].fmeasure)\n    rougeL_scores.append(scores['rougeL'].fmeasure)\n    rougeLsum_scores.append(scores['rougeLsum'].fmeasure)\n\n# Trung bình các chỉ số\navg_bleu = np.mean(bleu_scores)\navg_rouge1 = np.mean(rouge1_scores)\navg_rouge2 = np.mean(rouge2_scores)\navg_rougeL = np.mean(rougeL_scores)\navg_rougeLsum = np.mean(rougeLsum_scores)\n\n# In kết quả\nprint(\"\\n--- Evaluation Results ---\")\nprint(f\"BLEU: {avg_bleu:.2f}\")\nprint(f\"ROUGE-1: {avg_rouge1:.4f}\")\nprint(f\"ROUGE-2: {avg_rouge2:.4f}\")\nprint(f\"ROUGE-L: {avg_rougeL:.4f}\")\nprint(f\"ROUGE-Lsum: {avg_rougeLsum:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T03:13:04.865114Z","iopub.execute_input":"2025-05-03T03:13:04.865682Z","iopub.status.idle":"2025-05-03T03:13:50.768969Z","shell.execute_reply.started":"2025-05-03T03:13:04.865662Z","shell.execute_reply":"2025-05-03T03:13:50.768398Z"}},"outputs":[{"name":"stdout","text":"\n--- Evaluation Results ---\nBLEU: 6.64\nROUGE-1: 0.5054\nROUGE-2: 0.2493\nROUGE-L: 0.4181\nROUGE-Lsum: 0.4181\n","output_type":"stream"}],"execution_count":35}]}